{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyforest \n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brainstorming : \n",
      "\n",
      "1) Give user the option of choosing the country and job type. Will have to string format the link \n",
      "2) Some of the jobs have incomplete links. Will have to concat to make the link complete. Use regex and for loop to check if starts with https.\n",
      "3) Will have to set a time between scrape for each page and then for scraping of each link. Have to set cap at number of total jobs as well. \n",
      "4) Ask the user for the list of skills he want a summary. That way we can expand it to incorporate any job title and skill set. \n",
      "\n",
      "\n",
      "Final Workflow:\n",
      "\n",
      "1) Ask the user to input the following ; Job Title , Country, Skills Set\n",
      "2) Write a small script that gives back how many jobs on indeed.\n",
      "3) Ask the user to input how many jobs he wants to look through. Give a heads up that the more the jobs, the more the time it will take to run. Remember jobs are sorted by relevance by default.\n",
      "4) Start the script."
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cat 'project notes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import time\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def timer():\n",
    "    '''Calculate time it takes for process to complete\n",
    "    \n",
    "    Args:\n",
    "      None\n",
    "      \n",
    "    Yields:\n",
    "       float : the time in minutes for process to run\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    \n",
    "    yield\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    final_time = (end_time - start_time) / 60\n",
    "    \n",
    "    print(\"The time it took to scrape and compile the results was : {:.2f} minutes\".format(final_time))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello and welcome to the skills summariser. Let's get going\n",
      "\n",
      "Enter the job title you want to search for: data analyst \n",
      "-----------------------------\n",
      "Which country would you like to search in (no abbrevations please): united states\n",
      "-----------------------------\n",
      "Enter the skills you want a summary for and make sure you seperate each skill by a space: python sql excel tableau \n",
      "-----------------------------\n",
      "How many jobs do you want to look through. The more the jobs the longer the script might take: 5\n",
      "-----------------------------\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the skills you provided:\n",
      "---------------------------------\n",
      "The skill python appeared 39 times in 69 job postings or 56.52 of jobs require it\n",
      "The skill sql appeared 78 times in 69 job postings or 113.04 of jobs require it\n",
      "The skill excel appeared 83 times in 69 job postings or 120.29 of jobs require it\n",
      "The skill tableau appeared 20 times in 69 job postings or 28.99 of jobs require it\n",
      "The time it took to scrape and compile the results was : 0.95 minutes\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "print('Hello and welcome to the skills summariser. Let\\'s get going')\n",
    "print('')\n",
    "\n",
    "job_title = input('Enter the job title you want to search for: ')\n",
    "print('-----------------------------')\n",
    "country = input('Which country would you like to search in (no abbrevations please): ')\n",
    "print('-----------------------------')\n",
    "skills = input('Enter the skills you want a summary for and make sure you seperate each skill by a space: ')\n",
    "print('-----------------------------')\n",
    "n_jobs = input('How many jobs do you want to look through. The more the jobs the longer the script might take: ')\n",
    "print('-----------------------------')\n",
    "#initial cleaning \n",
    "\n",
    "job_title = job_title.title()\n",
    "country = country.title()\n",
    "skills = skills.lower().split()\n",
    "\n",
    "#'https://www.indeed.com/jobs?q=business+analyst&l=United+States'\n",
    "primary_link = 'https://www.indeed.com/jobs?q={0}&l={1}'\n",
    "further_pages_link = 'https://www.indeed.com/jobs?q={0}&l={1}&start={2}'\n",
    "\n",
    "\n",
    "with timer():\n",
    "    job_df = pd.DataFrame(columns = ['Job Title', 'Organization', 'Job Link'])\n",
    "    count = list(range(1,3))\n",
    "    start_range = list(range(10,50,10))\n",
    "    skills_dict = {}\n",
    "    titles = []\n",
    "    orgs = []\n",
    "    clean_links = []\n",
    "\n",
    "    \n",
    "    \n",
    "#     while job_df.shape[0] <= int(n_jobs):\n",
    "    for num in count:\n",
    "        if num == 1:\n",
    "            link = primary_link.format(job_title,country)\n",
    "            job_links_page = requests.get(link).text\n",
    "            first_page_parser = BeautifulSoup(job_links_page,'html.parser')\n",
    "            scraped_first_page = first_page_parser.body.find_all('div', attrs = {'data-tn-component':'organicJob'})\n",
    "\n",
    "            for job in scraped_first_page:\n",
    "                title = job.a.text.strip()\n",
    "                titles.append(title)\n",
    "\n",
    "                org = job.div.span.text.strip()\n",
    "                orgs.append(org)\n",
    "\n",
    "                link = job.a.get('href')\n",
    "\n",
    "                if link.startswith('https'):\n",
    "                    clean_links.append(link)\n",
    "                else:\n",
    "                    join_link = \"https://www.indeed.com\"+link\n",
    "                    clean_links.append(join_link)\n",
    "\n",
    "            job_df['Job Title'] = titles\n",
    "            job_df['Organization'] = orgs\n",
    "            job_df['Job Link'] = clean_links\n",
    "\n",
    "            for l in clean_links:\n",
    "                try:\n",
    "                    jd_request = requests.get(l)\n",
    "                    if jd_request.status_code == 200:\n",
    "                        jd_cont = jd_request.text\n",
    "                        job_page = BeautifulSoup(jd_cont,'html.parser')\n",
    "                        description = job_page.find('div', class_ = 'jobsearch-jobDescriptionText').text\n",
    "                        description = description.lower()\n",
    "\n",
    "                        for skill in skills:\n",
    "\n",
    "                            if skill in description:\n",
    "                                if skill in skills_dict:\n",
    "                                    skills_dict[skill] +=1\n",
    "                                else:\n",
    "                                    skills_dict[skill] = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #the code for scraping page 2 and forward starts from here\n",
    "\n",
    "        else:\n",
    "\n",
    "            temp_titles = []\n",
    "            temp_orgs = []\n",
    "            temp_links = []\n",
    "\n",
    "            for num in start_range:\n",
    "                link_cont = further_pages_link.format(job_title,country,num)\n",
    "                more_links_page = requests.get(link_cont).text\n",
    "                pages_parser = BeautifulSoup(more_links_page,'html.parser')\n",
    "                scraped_page = pages_parser.body.find_all('div', attrs = {'data-tn-component':'organicJob'})\n",
    "\n",
    "                for jobs in scraped_page:\n",
    "                    more_titles = jobs.a.text.strip()\n",
    "                    temp_titles.append(more_titles)\n",
    "\n",
    "                    more_orgs = jobs.div.span.text.strip()\n",
    "                    temp_orgs.append(more_orgs)\n",
    "\n",
    "                    more_links = jobs.a.get('href')\n",
    "\n",
    "                    if more_links.startswith('https'):\n",
    "                        temp_links.append(more_links)\n",
    "                    else:\n",
    "                        more_links = \"https://www.indeed.com\"+more_links\n",
    "                        temp_links.append(more_links)\n",
    "\n",
    "                    job_df = job_df.append({'Job Title':more_titles,'Organization': more_orgs, 'Job Link':more_links}, ignore_index = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                for l in temp_links:\n",
    "                    try:\n",
    "                        jd_request = requests.get(l)\n",
    "                        if jd_request.status_code == 200:\n",
    "                            jd_cont = jd_request.text\n",
    "                            job_page = BeautifulSoup(jd_cont,'html.parser')\n",
    "                            description = job_page.find('div', class_ = 'jobsearch-jobDescriptionText').text\n",
    "                            description = description.lower()\n",
    "\n",
    "                            for skill in skills:\n",
    "\n",
    "                                if skill in description:\n",
    "                                    if skill in skills_dict:\n",
    "                                        skills_dict[skill] += 1\n",
    "                                    else:\n",
    "                                        skills_dict[skill] = 1\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "    print('Based on the skills you provided:')\n",
    "    print('---------------------------------')\n",
    "    for sk in skills:\n",
    "        percentage = (float(skills_dict[sk]) / job_df.shape[0]) * 100\n",
    "        percentage = round(percentage,2)\n",
    "        print('The skill {0} appeared {1} times in {2} job postings or {3}% of jobs require it'.format(sk, skills_dict[sk], job_df.shape[0], percentage))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
